# Student Performance Indicator - End-to-End Machine Learning Project

A comprehensive MLOps project that predicts student math scores based on various demographic and educational factors. This project demonstrates a complete machine learning pipeline from data ingestion to model deployment.

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Problem Statement](#problem-statement)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Features](#features)
- [Technologies Used](#technologies-used)
- [Installation](#installation)
- [Usage](#usage)
- [Project Workflow](#project-workflow)
- [Components](#components)
- [Model Training](#model-training)
- [Logging](#logging)
- [Error Handling](#error-handling)

## ğŸ¯ Project Overview

This project implements an end-to-end machine learning pipeline to predict student math scores using various features such as gender, ethnicity, parental education level, lunch type, and test preparation course. The project follows MLOps best practices with modular code structure, comprehensive logging, and error handling.

## ğŸ“ Problem Statement

The goal of this project is to understand and predict how student performance (math scores) is affected by various factors including:
- **Gender**: Male/Female
- **Race/Ethnicity**: Group A, B, C, D, E
- **Parental Level of Education**: Various education levels
- **Lunch Type**: Standard/Free or Reduced
- **Test Preparation Course**: Completed/None
- **Reading Score**: Numerical feature
- **Writing Score**: Numerical feature

**Target Variable**: `math_score` (predicted value)

## ğŸ“Š Dataset

- **Source**: [Kaggle - Students Performance in Exams](https://www.kaggle.com/datasets/spscientist/students-performance-in-exams?datasetId=74977)
- **Size**: 1000 rows Ã— 8 columns
- **Location**: `notebook/data/stud.csv`

### Dataset Features

**Categorical Features:**
- `gender`: Student's gender
- `race_ethnicity`: Student's race/ethnicity group
- `parental_level_of_education`: Parent's education level
- `lunch`: Type of lunch (standard/free or reduced)
- `test_preparation_course`: Whether test prep course was completed

**Numerical Features:**
- `reading_score`: Student's reading score
- `writing_score`: Student's writing score
- `math_score`: Student's math score (target variable)

## ğŸ“ Project Structure

```
mlproject/
â”œâ”€â”€ artifacts/                 # Generated files (models, preprocessors, data splits)
â”‚   â”œâ”€â”€ preprocessor.pkl      # Saved preprocessing pipeline
â”‚   â”œâ”€â”€ model.pkl             # Trained model
â”‚   â”œâ”€â”€ train.csv             # Training dataset
â”‚   â”œâ”€â”€ test.csv              # Testing dataset
â”‚   â””â”€â”€ raw.csv               # Raw dataset
â”œâ”€â”€ logs/                      # Log files with timestamps
â”œâ”€â”€ notebook/                  # Jupyter notebooks for EDA and model training
â”‚   â”œâ”€â”€ 1. EDA STUDENT PERFORMANCE.ipynb
â”‚   â”œâ”€â”€ 2. MODEL TRAINING.ipynb
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ stud.csv
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ components/            # Core ML components
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py      # Data loading and train-test split
â”‚   â”‚   â”œâ”€â”€ data_transformation.py # Data preprocessing pipeline
â”‚   â”‚   â””â”€â”€ model_trainer.py       # Model training and evaluation
â”‚   â”œâ”€â”€ pipeline/             # Prediction and training pipelines
â”‚   â”‚   â”œâ”€â”€ train_pipeline.py # End-to-end training pipeline
â”‚   â”‚   â””â”€â”€ pred_pipeline.py  # Prediction pipeline
â”‚   â”œâ”€â”€ exception.py          # Custom exception handling
â”‚   â”œâ”€â”€ logger.py             # Logging configuration
â”‚   â””â”€â”€ utils.py              # Utility functions
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                 # Package setup configuration
â””â”€â”€ README.md                # Project documentation
```

## ğŸ”§ Technologies Used

- **Python 3.x**: Core programming language
- **Pandas**: Data manipulation and analysis
- **NumPy**: Numerical computations
- **Scikit-learn**: Machine learning algorithms and preprocessing
- **CatBoost**: Gradient boosting framework
- **XGBoost**: Extreme gradient boosting
- **Flask**: Web framework (for deployment)
- **Dill**: Object serialization
- **Matplotlib & Seaborn**: Data visualization (in notebooks)

## ğŸš€ Installation

### Prerequisites

- Python 3.7 or higher
- pip package manager

### Setup Instructions

1. **Clone the repository** (or navigate to the project directory):
   ```bash
   cd mlproject
   ```

2. **Create a virtual environment** (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Install the package in development mode**:
   ```bash
   pip install -e .
   ```

## ğŸ’» Usage

### Running the Data Ingestion Pipeline

```bash
python src/components/data_ingestion.py
```

This will:
- Load the raw data from `notebook/data/stud.csv`
- Split the data into train and test sets (80/20 split)
- Save the processed data to `artifacts/` directory

### Running the Data Transformation Pipeline

The data transformation is automatically triggered when running data ingestion. It will:
- Create preprocessing pipelines for numerical and categorical features
- Apply transformations to training and testing data
- Save the preprocessor object to `artifacts/preprocessor.pkl`

### Training the Model

```bash
python src/pipeline/train_pipeline.py
```

This will execute the complete training pipeline:
1. Data ingestion
2. Data transformation
3. Model training and evaluation
4. Model saving

### Making Predictions

```bash
python src/pipeline/pred_pipeline.py
```

## ğŸ”„ Project Workflow

```
Raw Data (stud.csv)
    â†“
Data Ingestion
    â”œâ”€â”€ Load data
    â”œâ”€â”€ Train-test split (80/20)
    â””â”€â”€ Save to artifacts/
    â†“
Data Transformation
    â”œâ”€â”€ Numerical Pipeline
    â”‚   â”œâ”€â”€ Imputation (median)
    â”‚   â””â”€â”€ Standard Scaling
    â”œâ”€â”€ Categorical Pipeline
    â”‚   â”œâ”€â”€ Imputation (most_frequent)
    â”‚   â”œâ”€â”€ One-Hot Encoding
    â”‚   â””â”€â”€ Standard Scaling
    â””â”€â”€ Save preprocessor
    â†“
Model Training
    â”œâ”€â”€ Train multiple models
    â”œâ”€â”€ Evaluate performance
    â”œâ”€â”€ Select best model
    â””â”€â”€ Save model
    â†“
Prediction Pipeline
    â””â”€â”€ Load model & preprocessor
    â””â”€â”€ Make predictions
```

## ğŸ§© Components

### 1. Data Ingestion (`data_ingestion.py`)

- **Class**: `DataIngestion`
- **Purpose**: Load raw data and split into train/test sets
- **Output**: Train and test CSV files in `artifacts/` directory

### 2. Data Transformation (`data_transformation.py`)

- **Class**: `DataTransformation`
- **Purpose**: Create and apply preprocessing pipelines
- **Features**:
  - Numerical features: Median imputation + Standard scaling
  - Categorical features: Most frequent imputation + One-hot encoding + Scaling
- **Output**: Preprocessed arrays and saved preprocessor object

### 3. Model Trainer (`model_trainer.py`)

- **Class**: `ModelTrainer`
- **Purpose**: Train, evaluate, and select the best model
- **Models**: Multiple algorithms including CatBoost, XGBoost, and others
- **Output**: Trained model saved as `artifacts/model.pkl`

### 4. Logging (`logger.py`)

- **Purpose**: Configure logging for the entire project
- **Features**:
  - Timestamped log files in `logs/` directory
  - Detailed logging with line numbers and timestamps
  - INFO level logging by default

### 5. Exception Handling (`exception.py`)

- **Class**: `CustomException`
- **Purpose**: Custom exception handling with detailed error messages
- **Features**: Includes file name, line number, and error message

### 6. Utilities (`utils.py`)

- **Function**: `save_object()`
- **Purpose**: Save Python objects (models, preprocessors) using dill
- **Usage**: Serializes objects to `.pkl` files

## ğŸ“ˆ Model Training

The project supports training multiple machine learning models:
- **CatBoost**: Gradient boosting with categorical features support
- **XGBoost**: Extreme gradient boosting
- **Other algorithms**: As configured in the model trainer

The best model is selected based on evaluation metrics (typically RÂ² score or RMSE) and saved for production use.

## ğŸ“ Logging

All operations are logged with timestamps:
- Log files are stored in `logs/` directory
- Format: `DD_MM_YYYY_HH_MM_SS.log`
- Includes: Timestamp, line number, module name, log level, and message

## âš ï¸ Error Handling

The project uses custom exception handling:
- All exceptions are caught and wrapped in `CustomException`
- Error messages include:
  - Python script name
  - Line number where error occurred
  - Detailed error message

## ğŸ‘¤ Author

**Varadaraj**
- Email: varadaraj.kamisetty@gmail.com

## ğŸ“„ License

This project is for educational purposes.

## ğŸ™ Acknowledgments

- Dataset source: [Kaggle - Students Performance in Exams](https://www.kaggle.com/datasets/spscientist/students-performance-in-exams?datasetId=74977)
- Built following MLOps best practices

## ğŸ“š Additional Notes

- The `setup.py` file allows the project to be installed as a package, making it reusable across different environments
- All artifacts (models, preprocessors, data splits) are saved in the `artifacts/` directory
- The project follows a modular structure for easy maintenance and scalability

---

**Note**: Make sure to update the data path in `data_ingestion.py` if your dataset location differs from the default path.
